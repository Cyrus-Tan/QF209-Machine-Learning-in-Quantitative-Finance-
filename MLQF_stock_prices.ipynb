{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSTZ2z-SgHG6"
      },
      "source": [
        "# PART 1: Simple Markowitz-Model implementation to estimate weightage of each stock in portfolio, where portfolio will have highest sharpe ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sra05nTEbSJk"
      },
      "outputs": [],
      "source": [
        "### PART 1: Simple Markowitz-Model implementation to estimate weightage of each stock in portfolio, where portfolio will have highest sharpe ratio\n",
        "\n",
        "# Step 1: Import modules and define constants\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize as optimization\n",
        "from scipy import interpolate\n",
        "from scipy import stats\n",
        "import plotly.express as px\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# On avge, 252 trading days a year\n",
        "NUM_TRADING_DAYS = 252\n",
        "\n",
        "# Generate random w (diff. portfolios)\n",
        "NUM_PORTFOLIOS = 10000\n",
        "\n",
        "# Our portfolio stocks\n",
        "stocks = ['AAPL', 'NVDA', 'MAR', 'LMT', 'BA']\n",
        "\n",
        "# Historical data - define START, END dates\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2023-10-01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC9dp-elggBB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Data retrieval and Visualisation\n",
        "def download_data():\n",
        "    # name of stock as key, stock values 2010-2023\n",
        "    stock_data = {}\n",
        "    for stock in stocks:\n",
        "        ticker = yf.Ticker(stock)\n",
        "        stock_data[stock] = ticker.history(start=start_date, end=end_date)['Close']\n",
        "    return pd.DataFrame(stock_data)\n",
        "\n",
        "def show_data(data):\n",
        "    data.plot(figsize=(10, 5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpje7_SM6MAx"
      },
      "outputs": [],
      "source": [
        "# Step 3: Returns calculation and Portfolio Statistics\n",
        "# To calculate return using normalisation, show statistics using annual metrics\n",
        "def calculate_return(data):\n",
        "    # Normalization - to measure variables in comparable metric\n",
        "    log_return = np.log(data / data.shift(1))\n",
        "    return log_return[1:]                    # Don't need first row of dataset, since values are NaN\n",
        "\n",
        "def show_statistics(returns):\n",
        "    # Instead of daily metrics, use annual metrics\n",
        "    annual_mean = returns.mean() * NUM_TRADING_DAYS\n",
        "    annual_covariance = returns.cov() * NUM_TRADING_DAYS\n",
        "    print(\"Annual Mean Returns:\")\n",
        "    print(annual_mean)\n",
        "    print(\"Annual Covariance Matrix:\")\n",
        "    print(annual_covariance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDeSuB2JrdZ"
      },
      "outputs": [],
      "source": [
        "# Step 4: To visaulise efficient frontier and displau portfolio metrics\n",
        "def show_mean_variance(returns, weights):\n",
        "    portfolio_return = np.sum(returns.mean() * weights) * NUM_TRADING_DAYS\n",
        "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * NUM_TRADING_DAYS, weights)))\n",
        "    print(\"Expected portfolio mean (return): \", portfolio_return)\n",
        "    print(\"Expected portfolio volatility (standard deviation): \", portfolio_volatility)\n",
        "\n",
        "\n",
        "# Generate multiple portfolios: Display their efficient frontier\n",
        "def show_portfolios(returns, volatilities):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(volatilities, returns, c=returns/volatilities, marker='o')\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Expected Volatility')\n",
        "    plt.ylabel('Expected Return')\n",
        "    plt.colorbar(label='Sharpe Ratio')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztHO5MWQ6RHC"
      },
      "outputs": [],
      "source": [
        "# Step 5: Portfolio Generation\n",
        "def generate_portfolios(returns):\n",
        "    portfolio_means = []\n",
        "    portfolio_risks = []\n",
        "    portfolio_weights = []\n",
        "\n",
        "    for _ in range(NUM_PORTFOLIOS):\n",
        "        # Generate random weight\n",
        "        w = np.random.random(len(stocks))\n",
        "        w /= np.sum(w)\n",
        "        portfolio_weights.append(w)\n",
        "        portfolio_mean = np.sum(returns.mean() * w) * NUM_TRADING_DAYS\n",
        "        portfolio_risk = np.sqrt(np.dot(w.T, np.dot(returns.cov() * NUM_TRADING_DAYS, w)))\n",
        "        portfolio_means.append(portfolio_mean)\n",
        "        portfolio_risks.append(portfolio_risk)\n",
        "\n",
        "    return np.array(portfolio_weights), np.array(portfolio_means), np.array(portfolio_risks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMJB_2We8-aN"
      },
      "outputs": [],
      "source": [
        "# Step 6: Portfolio Optimization Functions\n",
        "def statistics(weights, returns):\n",
        "    portfolio_return = np.sum(returns.mean() * weights) * NUM_TRADING_DAYS\n",
        "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * NUM_TRADING_DAYS, weights)))\n",
        "    return np.array([portfolio_return, portfolio_volatility, portfolio_return / portfolio_volatility])\n",
        "\n",
        "# Scipy optimize module can find the minimum of a given function\n",
        "# Maximum of f(x) is minimum of -f(x)\n",
        "def min_function_sharpe(weights, returns):\n",
        "    return -statistics(weights, returns)[2]\n",
        "\n",
        "# Constraints: Sum of weights = 1\n",
        "# f(x) = 0 this is the function to minimize\n",
        "def optimize_portfolio(weights, returns):\n",
        "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "    # Weights can be 1 at most: 1 when 100% of money invested in single stock\n",
        "    bounds = tuple((0, 1) for _ in range(len(stocks)))\n",
        "    return optimization.minimize(fun=min_function_sharpe, x0=weights[0], args=returns, method='SLSQP', bounds=bounds, constraints=constraints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF4cI6lO9I5v"
      },
      "outputs": [],
      "source": [
        "# Step 7: Portfolio Results Presentation\n",
        "def print_optimal_portfolio(optimum, returns):\n",
        "    print(\"Optimal portfolio weights:\", optimum['x'].round(3))\n",
        "    expected_return, volatility, sharpe_ratio = statistics(optimum['x'].round(3), returns)\n",
        "    print(f\"Expected return: {expected_return}\")\n",
        "    print(f\"Expected volatility (standard deviation): {volatility}\")\n",
        "    print(f\"Sharpe ratio: {sharpe_ratio}\")\n",
        "\n",
        "def show_optimal_portfolio(opt, rets, portfolio_rets, portfolio_vols):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(portfolio_vols, portfolio_rets, c=portfolio_rets / portfolio_vols, marker='o')\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Expected Volatility')\n",
        "    plt.ylabel('Expected Return')\n",
        "    plt.colorbar(label='Sharpe Ratio')\n",
        "    plt.plot(statistics(opt['x'], rets)[1], statistics(opt['x'], rets)[0], 'g*', markersize=20.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-eNCRJg9Nze",
        "outputId": "96880c84-7167-4d36-d557-4b5c3ab180be"
      },
      "outputs": [],
      "source": [
        "# Main Workflow\n",
        "# First output: Display stock prices of the 5 stocks, using yfinance\n",
        "dataset = download_data()\n",
        "show_data(dataset)\n",
        "print('\\n')\n",
        "\n",
        "# Second output: Annual Mean returns and covariance matrix of stocks in portfolio\n",
        "log_daily_returns = calculate_return(dataset)\n",
        "show_statistics(log_daily_returns)\n",
        "print('\\n')\n",
        "\n",
        "# Third output: Display Scatterplot of expected return against expected volatility, color coded by sharpe ratio\n",
        "weights, means, risks = generate_portfolios(log_daily_returns)\n",
        "show_portfolios(means, risks)\n",
        "print('\\n')\n",
        "\n",
        "# Fourth output: Display most optimal portfolio (highest sharpe ratio), denoted by green star\n",
        "# Each value in the list for optimal portfolio shows the corresponding optimal percentage each stock should take in portfolio\n",
        "optimum = optimize_portfolio(weights, log_daily_returns)\n",
        "print_optimal_portfolio(optimum, log_daily_returns)\n",
        "show_optimal_portfolio(optimum, log_daily_returns, means, risks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkEljuvSgU6H"
      },
      "source": [
        "# Part 2: Incorporating sentiment score of earnings call and technical indicators with stock data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcs1y3jU9RpR"
      },
      "outputs": [],
      "source": [
        "### PART 2: Analysing of the 5 stock prices, with incorporation of their corresponding sentiment score (extracted using NLP and FinBert)\n",
        "### NOTE: Wil not be forecasting prices because there are measured in absolute terms and thus harder to compare across time and other assets.\n",
        "###       Hence, will be forecasting daily returns instead\n",
        "\n",
        "# Step 1: Import modules and load data\n",
        "import yfinance as yf\n",
        "import math\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "bIGVfEfMg4Yj",
        "outputId": "46b0d4f5-2795-4a00-e9e4-f0d4a26466c6"
      },
      "outputs": [],
      "source": [
        "stocks = ['AAPL', 'NVDA', 'MAR', 'LMT', 'BA']\n",
        "# stocks = []\n",
        "stock_data = yf.download(stocks, start='2010-01-01', end='2023-10-20', group_by='tickers')\n",
        "stock_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "s0xRgk2RQAcX",
        "outputId": "903e0628-7073-404d-8394-b7744a8f9b56"
      },
      "outputs": [],
      "source": [
        "# Step 2: Clean and explore data\n",
        "stock_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-6nX0ziu7ZR",
        "outputId": "5a48b163-1565-4912-c872-10ae96a4281d"
      },
      "outputs": [],
      "source": [
        "# Check for presence of missing values\n",
        "missing_values_count = stock_data.isnull().sum()\n",
        "missing_values_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7th5XzrYQeX",
        "outputId": "14637113-c8a6-4223-cb15-19e8ade5eea6"
      },
      "outputs": [],
      "source": [
        "# Check shape of dataframe\n",
        "stock_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC3v0TZwZUKQ",
        "outputId": "d6f8d8aa-580f-4108-cbc2-ac38329ad287"
      },
      "outputs": [],
      "source": [
        "# Check columns of dataframe\n",
        "stock_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4aMWUd5ZqjV"
      },
      "outputs": [],
      "source": [
        "aapl = yf.Ticker('AAPL')\n",
        "\n",
        "# uncomment later\n",
        "# aapl.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HGEanWeMapSk",
        "outputId": "64f68639-9778-4a05-e026-5058cfef23f4"
      },
      "outputs": [],
      "source": [
        "# Explore data: Create line charts for columns of dataset\n",
        "# Define the DataFrame to plot (Will display line charts for 5 stocks separately here for ease of visualisation)\n",
        "aapl_df = yf.download('AAPL', start='2010-01-01', end='2023-10-20')\n",
        "nvda_df = yf.download('NVDA', start='2010-01-01', end='2023-10-20')\n",
        "mar_df = yf.download('MAR', start='2010-01-01', end='2023-10-20')\n",
        "lmt_df = yf.download('LMT', start='2010-01-01', end='2023-10-20')\n",
        "ba_df = yf.download('BA', start='2010-01-01', end='2023-10-20')\n",
        "\n",
        "# Function to plot line chart of every stock in stock_df (For a quick glance)\n",
        "def plot_line_charts(stock_df):\n",
        "    # Plot line charts\n",
        "    df_plot = stock_df.copy()\n",
        "\n",
        "    ncols = 2\n",
        "    nrows = int(round(df_plot.shape[1] / ncols, 0))\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(14, 7))\n",
        "    for i, ax in enumerate(fig.axes):\n",
        "        sns.lineplot(data = df_plot.iloc[:, i], ax=ax)\n",
        "        ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
        "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Line charts for AAPL: \\n\")\n",
        "plot_line_charts(aapl_df)\n",
        "print(\"Line charts for NVDA: \\n\")\n",
        "plot_line_charts(nvda_df)\n",
        "print(\"Line charts for MAR: \\n\")\n",
        "plot_line_charts(mar_df)\n",
        "print(\"Line charts for LMT: \\n\")\n",
        "plot_line_charts(lmt_df)\n",
        "print(\"Line charts for BA: \\n\")\n",
        "plot_line_charts(ba_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bfWXugW8o5MP",
        "outputId": "39d2da87-5e71-4561-b216-f336370b8faa"
      },
      "outputs": [],
      "source": [
        "# Step 3: Feature Engineering\n",
        "# Forecast daily returns by creating return column and analyse possible outliers using boxplot\n",
        "\n",
        "for stock in stocks:\n",
        "    stock_data[stock, 'Daily_Return'] = stock_data[stock]['Adj Close'].pct_change()\n",
        "    stock_data = stock_data.sort_index(axis=1)\n",
        "# print(stock_data)\n",
        "\n",
        "# Perform data analysis to identify possible outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "for stock in stocks:\n",
        "    sns.boxplot(x=stock_data[stock][\"Daily_Return\"])\n",
        "    plt.title(f'Box Plot for {stock} Daily Returns')\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "tvnROgC-4y9V",
        "outputId": "f3da5161-11a7-43fe-bb16-dc6bd3ad64a4"
      },
      "outputs": [],
      "source": [
        "# Since we feel that outliers are meaningful and representative of actual market conditions/events, we decide not to remove them\n",
        "# Using log transformation to reduce the impact of outliers, instead of removing them\n",
        "# Will be log tansforming the outliers to certain features in dataframe merged\n",
        "\n",
        "stock_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shjkZEcCGAGh"
      },
      "source": [
        "#Import Sentiment score from Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2MNpLBKYjP-N",
        "outputId": "f45a1c93-1efd-4d1a-bae4-4a7c1547f52b"
      },
      "outputs": [],
      "source": [
        "# Adding text sentiment score as an additional column to dataframe\n",
        "\n",
        "# Import excel data of sentiment score as dataframe\n",
        "df_sentiment_score = pd.read_excel(\"sentiment_score.xlsx\")\n",
        "df_sentiment_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8FLLHL-72UrG",
        "outputId": "c1957b03-d556-49b5-8092-cf30b0b26b32"
      },
      "outputs": [],
      "source": [
        "# Convert filename column, data will be in the format of YYYY-MM-DD.TickerName\n",
        "def convert_column(filename):\n",
        "    file_name_splitted = filename.split('.')\n",
        "    final_file_name = \"\"\n",
        "    month_to_numeric = {\"Jan\": \"01\",\n",
        "                        \"Feb\": \"02\",\n",
        "                        \"Mar\": \"03\",\n",
        "                        \"Apr\": \"04\",\n",
        "                        \"May\": \"05\",\n",
        "                        \"Jun\": \"06\",\n",
        "                        \"Jul\": \"07\",\n",
        "                        \"Aug\": \"08\",\n",
        "                        \"Sep\": \"09\",\n",
        "                        \"Oct\": \"10\",\n",
        "                        \"Nov\": \"11\",\n",
        "                        \"Dec\": \"12\"\n",
        "                        }\n",
        "\n",
        "    splitted_by_dash = file_name_splitted[0].split('-')\n",
        "    month_alphabet = splitted_by_dash[1]\n",
        "    month_numeric = month_to_numeric[month_alphabet]\n",
        "    final_file_name += splitted_by_dash[0] + \"-\" + month_numeric + \"-\" + splitted_by_dash[2] + \".\" + splitted_by_dash[3]\n",
        "\n",
        "    return final_file_name\n",
        "\n",
        "df_sentiment_score['filename'] = df_sentiment_score['filename'].apply(convert_column)\n",
        "df_sentiment_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-7eyPopCQuv"
      },
      "outputs": [],
      "source": [
        "# Function to make the date as row index, while column will be stock ticker\n",
        "def format_filename(row):\n",
        "    parts = row['filename'].split('.')\n",
        "    return pd.Series([parts[0], parts[1], row['sentiment_score']], index=['Date', 'Ticker', 'Sentiment_score'])\n",
        "\n",
        "# Apply the function to each row and restructure the DataFrame\n",
        "df_sentiment_score = df_sentiment_score.apply(format_filename, axis=1)\n",
        "\n",
        "# Set the 'Date' column as the row index\n",
        "df_sentiment_score.set_index('Date', inplace=True)\n",
        "# df_sentiment_score.groupby(\"Ticker\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O5dyICJg4Yk"
      },
      "outputs": [],
      "source": [
        "df_sentiment_score = df_sentiment_score.groupby(\"Ticker\")\n",
        "\n",
        "sentiment_score_dict = {}\n",
        "for stock in stocks:\n",
        "    suffixed_df = df_sentiment_score.get_group(f\"{stock}\").add_prefix(f'{stock}_')\n",
        "    suffixed_df.drop([f\"{stock}_Ticker\"], axis=1, inplace=True)\n",
        "    new_df = suffixed_df.reset_index('Date')\n",
        "    sentiment_score_dict[f'{stock}_df_sentiment_score'] = new_df\n",
        "\n",
        "# print(sentiment_score_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Qgbb-qg4Yk",
        "outputId": "4daecb0e-6429-48cc-fbd9-efe80da9294b"
      },
      "outputs": [],
      "source": [
        "sentiment_score_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "q6i92KPAGkUG",
        "outputId": "d872dcc3-6e45-4410-acf8-212f7acc692b"
      },
      "outputs": [],
      "source": [
        "stock_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rz_66t0g4Yl"
      },
      "outputs": [],
      "source": [
        "stock_data_dict = {}\n",
        "\n",
        "for stock in stocks:\n",
        "    # print(stock)\n",
        "    suffixed_df = stock_data[f'{stock}'].add_prefix(f'{stock}_')\n",
        "    new_df = suffixed_df.reset_index('Date')\n",
        "    # print(new_df)\n",
        "    new_df['Date'] = new_df['Date'].astype(str)\n",
        "    stock_data_dict[f'{stock}_stock_data'] = new_df\n",
        "\n",
        "# for key,value in stock_data_dict.items():\n",
        "#     print(key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U8zovNTg4Yl",
        "outputId": "db256810-8824-4fd9-91b1-b021bd5d678c"
      },
      "outputs": [],
      "source": [
        "print(stock_data_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckgBuK2kYcR1"
      },
      "source": [
        "# Merge sentiment_score dataframe with stock_data dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvTjSI6Ug4Yl"
      },
      "outputs": [],
      "source": [
        "# aapl_merged = aapl.merge(aaplSentiment, how='outer', on='Date')\n",
        "\n",
        "merged_dict = {}\n",
        "for stock in stocks:\n",
        "    new_merged_df = stock_data_dict[f'{stock}_stock_data'].merge(sentiment_score_dict[f'{stock}_df_sentiment_score'], how='outer', on='Date')\n",
        "    merged_dict[f\"{stock}\"] = new_merged_df\n",
        "\n",
        "# print(merged_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTs3HHr8g4Yl",
        "outputId": "80a7bb41-0825-4aeb-b050-d3d2fa82262f"
      },
      "outputs": [],
      "source": [
        "res = list(merged_dict.keys())[0]\n",
        "\n",
        "merged = pd.DataFrame(merged_dict[res])\n",
        "print(merged)\n",
        "\n",
        "for key, value in merged_dict.items():\n",
        "    if key != res:\n",
        "        # print(value)\n",
        "        merged = merged.merge(value, on='Date', how='outer')\n",
        "\n",
        "merged = merged.sort_values(by=['Date'], ascending=True)\n",
        "print(merged)\n",
        "# merged.to_excel('merged.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "-GjAIAr1jSVw",
        "outputId": "096ae246-10d0-426f-d19c-6d7382338b04"
      },
      "outputs": [],
      "source": [
        "# merged is a dataframe that has 1 additional row than dataframe stock_data because an earnings call by MAR was done on a Saturday (2019-05-11)\n",
        "merged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHlRiMa4Y6OO"
      },
      "source": [
        "# Data Cleaning, Exploratory Data Analysis and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No_GPYBiVW6f"
      },
      "outputs": [],
      "source": [
        "# I Will want to \"data clean\"/handle outliers for initial columns (e.g: 'Close'), because the technical indicators will need to use these values\n",
        "# I Will not be performing log transformation to handle outliers for columns: Daily_Return and Volume\n",
        "# -> Daily_Return (measured as % change in stock's price) can be both +ve and -ve and extreme returns can occur, log transformation not required\n",
        "# -> Although volume data can have outliers, it will be uesd as it is because unlike price data, it does not have a natural logarithmic interpretation. In fact, outliers in volume may carry valuable information about market events and trading behaviour\n",
        "\n",
        "# I will be performing log transformation to handle outliers in columns: Adj. CLose, Close, High, Low, Open\n",
        "# -> These columns are often right-skewed due to presence of extreme price values (stock splits, mergers, market events)\n",
        "# -> Applying log. transformation to these columns can mitigate impact of outliers and make data more normally distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GJOzttpPn9Nz",
        "outputId": "380229fb-3cca-498a-fddb-861db554611a"
      },
      "outputs": [],
      "source": [
        "# Using histogram to check data distribution if they are right-skewed (mean>median)\n",
        "\n",
        "# For Adj Close columns\n",
        "columns_to_plot = [\"AAPL_Adj Close\", \"NVDA_Adj Close\", \"MAR_Adj Close\", \"LMT_Adj Close\", \"BA_Adj Close\"]\n",
        "for column in columns_to_plot:\n",
        "    plt.hist(merged[column], bins=30, alpha=0.6, label=column)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histograms of AdjClose Columns')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identify outliers using Z-score and handle them using log transformation\n",
        "# Set a common threshold for identifying outliers\n",
        "threshold = 3  # indicates outliers are data points 3 standard deviations away from the mean\n",
        "\n",
        "for column_name in columns_to_plot:\n",
        "    # Calculate the Z-score for the current column\n",
        "    z_scores = np.abs(stats.zscore(merged[column_name]))\n",
        "\n",
        "    # Identify outliers based on the Z-score\n",
        "    outliers = merged[column_name][z_scores > threshold]\n",
        "\n",
        "    # Apply log transformation to outliers and replace in the DataFrame\n",
        "    merged.loc[z_scores > threshold, column_name] = np.log1p(outliers)\n",
        "\n",
        "    # Check the distribution after log transformation\n",
        "    sns.histplot(merged[column_name], kde=True)\n",
        "    plt.xlabel(column_name)\n",
        "    plt.title(f'Distribution of {column_name} after log transformation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aYsZL3pMVIni",
        "outputId": "a482b2c3-e6d1-418a-8273-d9bd953164fc"
      },
      "outputs": [],
      "source": [
        "# Using histogram to check data distribution if they are right-skewed (mean>median)\n",
        "\n",
        "# For Close columns\n",
        "columns_to_plot = [\"AAPL_Close\", \"NVDA_Close\", \"MAR_Close\", \"LMT_Close\", \"BA_Close\"]\n",
        "for column in columns_to_plot:\n",
        "    plt.hist(merged[column], bins=30, alpha=0.6, label=column)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histograms of Close Columns')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identify outliers using Z-score and handle them using log transformation\n",
        "# Set a common threshold for identifying outliers\n",
        "threshold = 3  # indicates outliers are data points 3 standard deviations away from the mean\n",
        "\n",
        "for column_name in columns_to_plot:\n",
        "    # Calculate the Z-score for the current column\n",
        "    z_scores = np.abs(stats.zscore(merged[column_name]))\n",
        "\n",
        "    # Identify outliers based on the Z-score\n",
        "    outliers = merged[column_name][z_scores > threshold]\n",
        "\n",
        "    # Apply log transformation to outliers and replace in the DataFrame\n",
        "    merged.loc[z_scores > threshold, column_name] = np.log1p(outliers)\n",
        "\n",
        "    # Check the distribution after log transformation\n",
        "    sns.histplot(merged[column_name], kde=True)\n",
        "    plt.xlabel(column_name)\n",
        "    plt.title(f'Distribution of {column_name} after log transformation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "md8GIHlhmmIn",
        "outputId": "1bfb00e8-21f6-41c0-f56f-f34d12d09646"
      },
      "outputs": [],
      "source": [
        "# Using histogram to check data distribution if they are right-skewed (mean>median)\n",
        "\n",
        "# For High columns\n",
        "columns_to_plot = [\"AAPL_High\", \"NVDA_High\", \"MAR_High\", \"LMT_High\", \"BA_High\"]\n",
        "for column in columns_to_plot:\n",
        "    plt.hist(merged[column], bins=30, alpha=0.6, label=column)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histograms of High Columns')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identify outliers using Z-score and handle them using log transformation\n",
        "# Set a common threshold for identifying outliers\n",
        "threshold = 3  # indicates outliers are data points 3 standard deviations away from the mean\n",
        "\n",
        "for column_name in columns_to_plot:\n",
        "    # Calculate the Z-score for the current column\n",
        "    z_scores = np.abs(stats.zscore(merged[column_name]))\n",
        "\n",
        "    # Identify outliers based on the Z-score\n",
        "    outliers = merged[column_name][z_scores > threshold]\n",
        "\n",
        "    # Apply log transformation to outliers and replace in the DataFrame\n",
        "    merged.loc[z_scores > threshold, column_name] = np.log1p(outliers)\n",
        "\n",
        "    # Check the distribution after log transformation\n",
        "    sns.histplot(merged[column_name], kde=True)\n",
        "    plt.xlabel(column_name)\n",
        "    plt.title(f'Distribution of {column_name} after log transformation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ExYLVKD9bLJ1",
        "outputId": "c9c77a52-797c-4767-91d3-7f9b8d70df3a"
      },
      "outputs": [],
      "source": [
        "# Using histogram to check data distribution if they are right-skewed (mean>median)\n",
        "\n",
        "# For Low columns\n",
        "columns_to_plot = [\"AAPL_Low\", \"NVDA_Low\", \"MAR_Low\", \"LMT_Low\", \"BA_Low\"]\n",
        "for column in columns_to_plot:\n",
        "    plt.hist(merged[column], bins=30, alpha=0.6, label=column)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histograms of Low Columns')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identify outliers using Z-score and handle them using log transformation\n",
        "# Set a common threshold for identifying outliers\n",
        "threshold = 3  # indicates outliers are data points 3 standard deviations away from the mean\n",
        "\n",
        "for column_name in columns_to_plot:\n",
        "    # Calculate the Z-score for the current column\n",
        "    z_scores = np.abs(stats.zscore(merged[column_name]))\n",
        "\n",
        "    # Identify outliers based on the Z-score\n",
        "    outliers = merged[column_name][z_scores > threshold]\n",
        "\n",
        "    # Apply log transformation to outliers and replace in the DataFrame\n",
        "    merged.loc[z_scores > threshold, column_name] = np.log1p(outliers)\n",
        "\n",
        "    # Check the distribution after log transformation\n",
        "    sns.histplot(merged[column_name], kde=True)\n",
        "    plt.xlabel(column_name)\n",
        "    plt.title(f'Distribution of {column_name} after log transformation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1SIOW3obK_V",
        "outputId": "07fd98fd-5cc6-4662-e682-f930f86f150d"
      },
      "outputs": [],
      "source": [
        "# Using histogram to check data distribution if they are right-skewed (mean>median)\n",
        "\n",
        "# For High columns\n",
        "columns_to_plot = [\"AAPL_Open\", \"NVDA_Open\", \"MAR_Open\", \"LMT_Open\", \"BA_Open\"]\n",
        "for column in columns_to_plot:\n",
        "    plt.hist(merged[column], bins=30, alpha=0.6, label=column)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histograms of Open Columns')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identify outliers using Z-score and handle them using log transformation\n",
        "# Set a common threshold for identifying outliers\n",
        "threshold = 3  # indicates outliers are data points 3 standard deviations away from the mean\n",
        "\n",
        "for column_name in columns_to_plot:\n",
        "    # Calculate the Z-score for the current column\n",
        "    z_scores = np.abs(stats.zscore(merged[column_name]))\n",
        "\n",
        "    # Identify outliers based on the Z-score\n",
        "    outliers = merged[column_name][z_scores > threshold]\n",
        "\n",
        "    # Apply log transformation to outliers and replace in the DataFrame\n",
        "    merged.loc[z_scores > threshold, column_name] = np.log1p(outliers)\n",
        "\n",
        "    # Check the distribution after log transformation\n",
        "    sns.histplot(merged[column_name], kde=True)\n",
        "    plt.xlabel(column_name)\n",
        "    plt.title(f'Distribution of {column_name} after log transformation')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "Wj63QkVbfVR1",
        "outputId": "10711707-2566-4446-a974-568e1cb2db04"
      },
      "outputs": [],
      "source": [
        "merged        # merged dataframe outliers will now be replaced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY-w28OTeqKg",
        "outputId": "d6c7339a-6459-4d5b-8db5-ddd57018997b"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in any of the columns (except sentiment_score)\n",
        "# 1st NaN row: first row for Daily_Return  (because there's no return on 'first day')\n",
        "# 2nd NaN row: I have found out that the NaN value comes from the row 2356 in excel => which is due to: an earnings call by MAR was done on a Saturday (2019-05-11)\n",
        "\n",
        "# Will be using zero fill to replace (Assume no return on first day) 1st NaN row for \"Daily_Return\" columns, Using Median to replace 1st NaN row of other columns,\n",
        "\n",
        "# Check for NaN values in all columns of the DataFrame and count them\n",
        "nan_counts = merged.isna().sum()\n",
        "\n",
        "# Display columns with NaN values and the count of NaN values\n",
        "columns_with_nan = nan_counts[nan_counts > 0]\n",
        "print(columns_with_nan)\n",
        "\n",
        "\n",
        "zero_fill_columns = [\"AAPL_Daily_Return\", \"NVDA_Daily_Return\", \"MAR_Daily_Return\", \"LMT_Daily_Return\", \"BA_Daily_Return\"]\n",
        "for column in zero_fill_columns:\n",
        "    if merged[column].isna().any():\n",
        "        first_nan_index = merged[column].index[merged[column].isna()].tolist()[0]         # Only for the first row of Daily_Return columns\n",
        "\n",
        "        merged.at[first_nan_index, column] = 0\n",
        "\n",
        "\n",
        "# Display columns with NaN values and the count of NaN values\n",
        "nan_counts = merged.isna().sum()\n",
        "columns_with_nan = nan_counts[nan_counts > 0]\n",
        "print(columns_with_nan)\n",
        "\n",
        "columns_to_replace_outliers = [\"AAPL_Adj Close\", \"AAPL_Close\", \"AAPL_Daily_Return\", \"AAPL_High\", \"AAPL_Low\", \"AAPL_Open\", \"AAPL_Volume\",\n",
        "                               \"NVDA_Adj Close\", \"NVDA_Close\", \"NVDA_Daily_Return\", \"NVDA_High\", \"NVDA_Low\", \"NVDA_Open\", \"NVDA_Volume\",\n",
        "                               \"MAR_Adj Close\", \"MAR_Close\", \"MAR_Daily_Return\", \"MAR_High\", \"MAR_Low\", \"MAR_Open\", \"MAR_Volume\",\n",
        "                               \"LMT_Adj Close\", \"LMT_Close\", \"LMT_Daily_Return\", \"LMT_High\", \"LMT_Low\", \"LMT_Open\", \"LMT_Volume\",\n",
        "                               \"BA_Adj Close\", \"BA_Close\", \"BA_Daily_Return\", \"BA_High\", \"BA_Low\", \"BA_Open\", \"BA_Volume\",\n",
        "                               ]\n",
        "\n",
        "\n",
        "# Function that replaces NaN values of column with median\n",
        "def replace_nan_with_median(df, columns_to_process):\n",
        "    for column in columns_to_process:\n",
        "        median = df[column].median()\n",
        "        df[column].fillna(median, inplace=True)\n",
        "    return df\n",
        "\n",
        "merged = replace_nan_with_median(merged, columns_to_replace_outliers)\n",
        "\n",
        "print(merged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiFdyNAroHps",
        "outputId": "6036916f-458e-4889-82f7-850fc1278d8f"
      },
      "outputs": [],
      "source": [
        "# Interpolate the sentiment scores for all 5 stocks for those non earnings call dates using linear interpolation\n",
        "columns_to_interpolate = ['AAPL_Sentiment_score', 'NVDA_Sentiment_score', 'MAR_Sentiment_score', 'LMT_Sentiment_score', 'BA_Sentiment_score']  # Replace with your column names\n",
        "for column in columns_to_interpolate:\n",
        "    merged[column].interpolate(method='linear', inplace=True)\n",
        "\n",
        "merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OPTWMhbm3m6",
        "outputId": "82b4b357-96fb-41dd-c58e-2df696d06655"
      },
      "outputs": [],
      "source": [
        "#Display columns with NaN values and the count of NaN values\n",
        "nan_counts = merged.isna().sum()\n",
        "columns_with_nan = nan_counts[nan_counts > 0]\n",
        "print(columns_with_nan)\n",
        "\n",
        "# Since there are still leftover NaN for sentiment score column, possibly due to presence of consecutive NaN values, I will use backfill to handle them\n",
        "columns_to_backfill = ['AAPL_Sentiment_score', 'NVDA_Sentiment_score', 'MAR_Sentiment_score', 'LMT_Sentiment_score', 'BA_Sentiment_score']\n",
        "for column in columns_to_backfill:\n",
        "    merged[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "merged.tail(50)\n",
        "\n",
        "# Confirm there is no leftover NaN values\n",
        "nan_counts_new = merged.isna().sum()\n",
        "columns_with_nan_new = nan_counts_new[nan_counts > 0]\n",
        "print(columns_with_nan_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "NkF9XEsavuKJ",
        "outputId": "db475ee7-b821-4acd-a555-bd86a5dd9db0"
      },
      "outputs": [],
      "source": [
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "QXuw7IeHobsl",
        "outputId": "51b40181-4f55-4f55-b672-dbbc8a74f168"
      },
      "outputs": [],
      "source": [
        "# Add additional features: Technical indicators to measure price momentum\n",
        "\n",
        "portfolio_stocks = [\"AAPL\", \"NVDA\", \"MAR\", \"LMT\", \"BA\"]\n",
        "\n",
        "def add_technical_indicators(df, stock_symbol):\n",
        "    # Calculate historical volatility\n",
        "    df[f'{stock_symbol}_Volatility'] = df[f'{stock_symbol}_Close'].rolling(window=21).std()\n",
        "\n",
        "    # Calculate Simple Moving Average (SMA)\n",
        "    df[f'{stock_symbol}_SMA'] = df[f'{stock_symbol}_Close'].rolling(window=14).mean()\n",
        "\n",
        "    # Calculate Upper Bollinger Band\n",
        "    df[f'{stock_symbol}_Upper_Bollinger'] = df[f'{stock_symbol}_SMA'] + 2 * df[f'{stock_symbol}_Volatility']\n",
        "\n",
        "    # Calculate Lower Bollinger Band\n",
        "    df[f'{stock_symbol}_Lower_Bollinger'] = df[f'{stock_symbol}_SMA'] - 2 * df[f'{stock_symbol}_Volatility']\n",
        "\n",
        "    # Calculate Moving Average Convergence Divergence (MACD)\n",
        "    df[f'{stock_symbol}_MACD'] = df[f'{stock_symbol}_Close'].rolling(window=12).mean() - df[f'{stock_symbol}_Close'].rolling(window=26).mean()\n",
        "\n",
        "    # Calculate Relative Strength Index (RSI)\n",
        "    delta = df[f'{stock_symbol}_Close'].diff(1)\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "\n",
        "    avg_gain = gain.rolling(window=14).mean()\n",
        "    avg_loss = loss.rolling(window=14).mean()\n",
        "\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    df[f'{stock_symbol}_RSI'] = rsi\n",
        "\n",
        "for each_stock in portfolio_stocks:\n",
        "    add_technical_indicators(merged, each_stock)\n",
        "\n",
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7loE3vz2CzF",
        "outputId": "e70d9496-1258-44ab-f9ed-bac10bec3159"
      },
      "outputs": [],
      "source": [
        "# Check NaN counts for the additional technical indicators columns\n",
        "# Display columns with NaN values and the count of NaN values\n",
        "nan_counts = merged.isna().sum()\n",
        "columns_with_nan = nan_counts[nan_counts > 0]\n",
        "print(columns_with_nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN5SGS1hTK5d",
        "outputId": "9fb4dff0-59ba-4dc5-f101-2e6fef881419"
      },
      "outputs": [],
      "source": [
        "# Using backfill interpolation to fill up the NaN values for the above columns, useful for consecutive NaN values\n",
        "nan_technical_indicators = merged.isna()\n",
        "for column in nan_technical_indicators:\n",
        "    merged[column].interpolate(method='bfill', inplace=True)\n",
        "\n",
        "# Check if there are still NaN values\n",
        "new_nan_counts = merged.isna().sum()\n",
        "new_columns_with_nan = new_nan_counts[new_nan_counts > 0]\n",
        "print(new_columns_with_nan)\n",
        "print('\\n')\n",
        "\n",
        "# Confrim no leftover NaN values for all columns in dataframe\n",
        "print(merged.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.sort_index(axis=1, inplace=True)\n",
        "merged.set_index(\"Date\", inplace = True)\n",
        "merged.to_excel('merged.xlsx', index=True)\n",
        "# merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYYEVMU_kU69"
      },
      "outputs": [],
      "source": [
        "# Standardize features so that they have mean of 0 and std of 1, ensure all features are on a similar scale\n",
        "# Define the columns you want to standardize (excluding non-numeric columns)\n",
        "columns_to_standardize = merged.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the selected columns\n",
        "merged[columns_to_standardize] = scaler.fit_transform(merged[columns_to_standardize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "hrKENpdFlbr0",
        "outputId": "3b51ecbb-e2e0-4368-a20e-8bff4e97663b"
      },
      "outputs": [],
      "source": [
        "merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfyCl9syX0LC"
      },
      "source": [
        "#Performing Data Visualization to provide context for feature selection process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjiXOEoMXzeC",
        "outputId": "61a648d4-83bd-4bc7-8327-df7366de403a"
      },
      "outputs": [],
      "source": [
        "# See all columns in dataframe\n",
        "merged.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmfNRuzncB4d"
      },
      "outputs": [],
      "source": [
        "### TO BE EDITED: After dataframe is back to multiindex, then can use plotly.express to plot interactive scatter matrix to see relationship of columns for each stock\n",
        "\n",
        "# Select a subset of columns for visualization => Correlation Analysis\n",
        "# selected_columns = [\n",
        "#     'AAPL_Adj Close', 'AAPL_Close', 'AAPL_Daily_Return', 'AAPL_High', 'AAPL_Low', 'AAPL_Open',\n",
        "#     'NVDA_Adj Close', 'NVDA_Close', 'NVDA_Daily_Return', 'NVDA_High', 'NVDA_Low', 'NVDA_Open',\n",
        "#     'MAR_Adj Close', 'MAR_Close', 'MAR_Daily_Return', 'MAR_High', 'MAR_Low', 'MAR_Open'\n",
        "# ]\n",
        "\n",
        "# Create an interactive scatter matrix\n",
        "\n",
        "\n",
        "# fig = px.scatter_matrix(merged, dimensions=selected_columns, color=\"AAPL_Adj Close\", title=\"Stock Data Relationships\")\n",
        "# fig.update_traces(marker=dict(size=3), selector=dict(mode='markers'))\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JvHd-HNeotn"
      },
      "source": [
        "# Feature Selection using xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4ly98QUyfec"
      },
      "outputs": [],
      "source": [
        "# Using xgboost model for feature selection, to select most important features for LSTM Model training\n",
        "feature_columns_AAPL = ['AAPL_Adj Close', 'AAPL_Close', 'AAPL_Daily_Return',\n",
        "       'AAPL_High', 'AAPL_Low', 'AAPL_Open', 'AAPL_Volume',\n",
        "       'AAPL_Sentiment_score', 'AAPL_Volatility', 'AAPL_SMA',\n",
        "       'AAPL_Upper_Bollinger', 'AAPL_Lower_Bollinger', 'AAPL_MACD',\n",
        "       'AAPL_RSI']\n",
        "\n",
        "feature_columns_NVDA = ['NVDA_Adj Close', 'NVDA_Close',\n",
        "       'NVDA_Daily_Return', 'NVDA_High', 'NVDA_Low', 'NVDA_Open',\n",
        "       'NVDA_Volume', 'NVDA_Sentiment_score', 'NVDA_Volatility', 'NVDA_SMA', 'NVDA_Upper_Bollinger',\n",
        "       'NVDA_Lower_Bollinger', 'NVDA_MACD', 'NVDA_RSI']\n",
        "\n",
        "\n",
        "feature_columns_MAR = ['MAR_Adj Close',\n",
        "       'MAR_Close', 'MAR_Daily_Return', 'MAR_High', 'MAR_Low', 'MAR_Open',\n",
        "       'MAR_Volume', 'MAR_Sentiment_score', 'MAR_Volatility',\n",
        "       'MAR_SMA', 'MAR_Upper_Bollinger', 'MAR_Lower_Bollinger',\n",
        "       'MAR_MACD', 'MAR_RSI']\n",
        "\n",
        "\n",
        "feature_columns_LMT = [ 'LMT_Adj Close', 'LMT_Close',\n",
        "       'LMT_Daily_Return', 'LMT_High', 'LMT_Low', 'LMT_Open',\n",
        "       'LMT_Volume', 'LMT_Sentiment_score',  'LMT_Volatility', 'LMT_SMA',\n",
        "       'LMT_Upper_Bollinger', 'LMT_Lower_Bollinger', 'LMT_MACD',\n",
        "       'LMT_RSI']\n",
        "\n",
        "\n",
        "feature_columns_BA = ['BA_Adj Close', 'BA_Close',\n",
        "       'BA_Daily_Return', 'BA_High', 'BA_Low', 'BA_Open', 'BA_Volume',\n",
        "       'BA_Sentiment_score', 'BA_Volatility', 'BA_SMA', 'BA_Upper_Bollinger',\n",
        "       'BA_Lower_Bollinger', 'BA_MACD', 'BA_RSI']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "Sf0jfmk4sqUN",
        "outputId": "e2a90997-d8d9-4f55-d2f7-d2f6e6bf4f5f"
      },
      "outputs": [],
      "source": [
        "# For AAPL\n",
        "X = merged[feature_columns_AAPL]\n",
        "\n",
        "# Train an XGBoost Regressor\n",
        "model = XGBRegressor()\n",
        "model.fit(X, X)  # Use the same features for both X and y => Unsupervised feature selection\n",
        "\n",
        "# Plot feature importances\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_importance(model, ax=ax)\n",
        "\n",
        "# Adjust font size and spacing for y-axis labels\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()  # Prevent labels from being cut off\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "i3wqnl2astJ_",
        "outputId": "7f7d53b3-db2e-4406-ab7a-4217c7769c8a"
      },
      "outputs": [],
      "source": [
        "# For NVDA\n",
        "X = merged[feature_columns_NVDA]\n",
        "\n",
        "# Train an XGBoost Regressor\n",
        "model = XGBRegressor()\n",
        "model.fit(X, X)  # Use the same features for both X and y => Unsupervised feature selection\n",
        "\n",
        "# Plot feature importances\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_importance(model, ax=ax)\n",
        "\n",
        "# Adjust font size and spacing for y-axis labels\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()  # Prevent labels from being cut off\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "z5NB16M3ss_J",
        "outputId": "e9c4b374-c237-4d19-c5c8-0daf409ad9e5"
      },
      "outputs": [],
      "source": [
        "# For MAR\n",
        "X = merged[feature_columns_MAR]\n",
        "\n",
        "# Train an XGBoost Regressor\n",
        "model = XGBRegressor()\n",
        "model.fit(X, X)  # Use the same features for both X and y => Unsupervised feature selection\n",
        "\n",
        "# Plot feature importances\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_importance(model, ax=ax)\n",
        "\n",
        "# Adjust font size and spacing for y-axis labels\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()  # Prevent labels from being cut off\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "us7T2w9tsqPN",
        "outputId": "79f17bd4-a3ed-4b5e-dd8c-df2e615dee0b"
      },
      "outputs": [],
      "source": [
        "# For LMT\n",
        "X = merged[feature_columns_LMT]\n",
        "\n",
        "# Train an XGBoost Regressor\n",
        "model = XGBRegressor()\n",
        "model.fit(X, X)  # Use the same features for both X and y => Unsupervised feature selection\n",
        "\n",
        "# Plot feature importances\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_importance(model, ax=ax)\n",
        "\n",
        "# Adjust font size and spacing for y-axis labels\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()  # Prevent labels from being cut off\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "k0VRK4eAsqKw",
        "outputId": "96f66b52-6fa6-4d52-9464-8c5d1c450278"
      },
      "outputs": [],
      "source": [
        "# For BA\n",
        "X = merged[feature_columns_BA]\n",
        "\n",
        "# Train an XGBoost Regressor\n",
        "model = XGBRegressor()\n",
        "model.fit(X, X)  # Use the same features for both X and y => Unsupervised feature selection\n",
        "\n",
        "# Plot feature importances\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_importance(model, ax=ax)\n",
        "\n",
        "# Adjust font size and spacing for y-axis labels\n",
        "plt.yticks(fontsize=8)\n",
        "plt.tight_layout()  # Prevent labels from being cut off\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yGK75WApZGA"
      },
      "outputs": [],
      "source": [
        "# LSTM Model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
